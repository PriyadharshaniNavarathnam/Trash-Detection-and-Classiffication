{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Computer vision miniproject**(2020/E/031,2020/E/120)\n",
        "2. object detection\n"
      ],
      "metadata": {
        "id": "napQl1dYyVKE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "01. Data set preparation\n",
        "Convert the video into frame.\n",
        "\n"
      ],
      "metadata": {
        "id": "ve2kdZzkyoq-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0QQT3W8n9pk",
        "outputId": "c63c963c-0d54-4688-b700-07156a0b228f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted 0 frames.\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import os\n",
        "\n",
        "# Define path to your video file\n",
        "video_path = '/path/to/your/video/file.mp4'\n",
        "output_dir = '/path/to/save/frames/'\n",
        "\n",
        "# Open the video file\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "frame_count = 0\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Save frames every 10th frame (or change interval as needed)\n",
        "    if frame_count % 10 == 0:\n",
        "        frame_filename = os.path.join(output_dir, f\"frame_{frame_count}.jpg\")\n",
        "        cv2.imwrite(frame_filename, frame)\n",
        "\n",
        "    frame_count += 1\n",
        "\n",
        "cap.release()\n",
        "print(f\"Extracted {frame_count} frames.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "z-sW3PzAyvoF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#mount drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMQNoZO5xstL",
        "outputId": "1cab9eff-b335-447e-cc9c-085862c56bdd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Load the dataset and annotation\n",
        "Aonnatate by using makesense.ai tool."
      ],
      "metadata": {
        "id": "YWzFC9tAzBeX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "import json\n",
        "\n",
        "# Paths to dataset\n",
        "images_path = '/content/drive/MyDrive/computer vision/object_detection/images'\n",
        "annotation_path = '/content/drive/MyDrive/computer vision/object_detection/annotation.json'\n",
        "\n",
        "# Load annotations\n",
        "with open(annotation_path, 'r') as f:\n",
        "    annotations = json.load(f)"
      ],
      "metadata": {
        "id": "WF_uPHwr9Rbb"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Convert the dataset and anntation file into expected trin model format."
      ],
      "metadata": {
        "id": "GbI8ai7WzVUP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract image filenames and labels\n",
        "image_files = []\n",
        "labels = []\n",
        "annotations_dict = {item['id']: item['name'] for item in annotations['categories']}\n",
        "\n",
        "for annotation in annotations['annotations']:\n",
        "    image_id = annotation['image_id']\n",
        "    image_file = os.path.join(images_path, annotations['images'][image_id - 1]['file_name'])\n",
        "    category_id = annotation['category_id']\n",
        "    image_files.append(image_file)\n",
        "    labels.append(category_id - 1)  # Adjust category ID to zero-based index\n"
      ],
      "metadata": {
        "id": "sxn9qEjr9Y9X"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Create model structure"
      ],
      "metadata": {
        "id": "7WYFhRDTzqFe"
      }
    },
    {
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "# Assuming 'image_files' and 'labels' are defined from previous steps\n",
        "\n",
        "# Load and preprocess images\n",
        "image_data = []\n",
        "for image_file in image_files:\n",
        "    image = cv2.imread(image_file)\n",
        "    image = cv2.resize(image, (224, 224))  # Resize to match model input shape\n",
        "    image_data.append(image)\n",
        "\n",
        "# Convert to NumPy arrays\n",
        "image_data = np.array(image_data)\n",
        "labels = np.array(labels)\n",
        "\n",
        "# Split data into training and validation sets\n",
        "train_data, val_data, train_labels, val_labels = train_test_split(\n",
        "    image_data, labels, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Normalize pixel values to the range [0, 1]\n",
        "train_data = train_data / 255.0\n",
        "val_data = val_data / 255.0\n",
        "\n",
        "# Create a simple sequential model (example)\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Flatten(input_shape=(224, 224, 3)), # Adjust input shape if needed\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(10, activation='softmax') # Adjust num_classes if needed\n",
        "])\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',  # Use sparse since labels are integers\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "history = model.fit(train_data, train_labels,\n",
        "                    epochs=20,\n",
        "                    validation_data=(val_data, val_labels))"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNneS2lt1Vm_",
        "outputId": "3a7a0415-0b43-4f48-c0ab-c52150476c6a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3s/step - accuracy: 0.2300 - loss: 23.0190 - val_accuracy: 0.6923 - val_loss: 66.2515\n",
            "Epoch 2/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - accuracy: 0.8172 - loss: 28.6669 - val_accuracy: 0.7692 - val_loss: 9.3740\n",
            "Epoch 3/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.7390 - loss: 6.9362 - val_accuracy: 0.9231 - val_loss: 4.7405\n",
            "Epoch 4/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.9634 - loss: 2.2057 - val_accuracy: 0.9231 - val_loss: 0.8117\n",
            "Epoch 5/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9869 - loss: 1.4000 - val_accuracy: 0.9231 - val_loss: 0.1022\n",
            "Epoch 6/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.9869 - loss: 1.0526 - val_accuracy: 0.9231 - val_loss: 1.5997\n",
            "Epoch 7/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.9765 - loss: 1.0356 - val_accuracy: 0.9231 - val_loss: 4.9214\n",
            "Epoch 8/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9634 - loss: 1.2373 - val_accuracy: 0.9231 - val_loss: 5.1028\n",
            "Epoch 9/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.9634 - loss: 0.7561 - val_accuracy: 0.9231 - val_loss: 1.9750\n",
            "Epoch 10/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 11/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 12/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.9869 - loss: 0.1602 - val_accuracy: 1.0000 - val_loss: 2.4204e-04\n",
            "Epoch 13/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 1.0000 - loss: 6.1649e-04 - val_accuracy: 1.0000 - val_loss: 9.1699e-09\n",
            "Epoch 14/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 15/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 16/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 17/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 18/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 19/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
            "Epoch 20/20\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 1.0000 - loss: 0.0000e+00 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained model\n",
        "model.save('/content/drive/MyDrive/computer vision/object_detection/mobilenet_model.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rG2OIa1E_RQW",
        "outputId": "87b5c3ba-6742-4ea1-fe0a-2bd97392d049"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Evaluvation and testing"
      ],
      "metadata": {
        "id": "cEADYKZqISFD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "\n",
        "# Paths\n",
        "video_path = '/content/drive/MyDrive/computer vision/Dataset2.avi'  # Replace with the path to your test video\n",
        "frames_output_path = '/content/drive/MyDrive/computer vision/object_detection/video_frames'  # Replace with the output folder\n",
        "\n",
        "# Ensure the output folder exists\n",
        "os.makedirs(frames_output_path, exist_ok=True)\n",
        "\n",
        "# Extract frames from the video\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "frame_count = 0\n",
        "\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "    frame_filename = os.path.join(frames_output_path, f'frame_{frame_count:04d}.jpg')\n",
        "    cv2.imwrite(frame_filename, frame)  # Save the frame\n",
        "    frame_count += 1\n",
        "\n",
        "cap.release()\n",
        "print(f\"Extracted {frame_count} frames to {frames_output_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVP37KRABY3-",
        "outputId": "f30b6aef-17d7-447b-e952-54a8e2c363a1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted 71 frames to /content/drive/MyDrive/computer vision/object_detection/video_frames\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Paths\n",
        "test_frames_path = '/content/drive/MyDrive/computer vision/object_detection/video_frames'\n",
        "model_path = '/content/drive/MyDrive/computer vision/object_detection/mobilenet_model.h5'\n",
        "output_path = '/content/drive/MyDrive/computer vision/object_detection/test_output'\n",
        "\n",
        "# Load the trained model\n",
        "model = tf.keras.models.load_model(model_path)\n",
        "\n",
        "# Create output directory if it doesn't exist\n",
        "os.makedirs(output_path, exist_ok=True)\n",
        "\n",
        "# Load label map (mapping category IDs to labels)\n",
        "annotations_path = '/content/drive/MyDrive/computer vision/object_detection/annotation.json'\n",
        "with open(annotations_path, 'r') as f:\n",
        "    annotations = json.load(f)\n",
        "\n",
        "annotations_dict = {item['id']: item['name'] for item in annotations['categories']}\n",
        "\n",
        "# Function to preprocess image\n",
        "def preprocess_image(image_path):\n",
        "    image = cv2.imread(image_path)\n",
        "    image = cv2.resize(image, (224, 224))  # Resize image to match model input shape\n",
        "    image = np.expand_dims(image, axis=0)  # Add batch dimension\n",
        "    image = image / 255.0  # Normalize image\n",
        "    return image\n",
        "\n",
        "# Function to draw bounding boxes on image\n",
        "def draw_bounding_boxes(image, boxes, class_ids, scores):\n",
        "    for i in range(len(boxes)):\n",
        "        box = boxes[i]\n",
        "        class_id = class_ids[i]\n",
        "        score = scores[i]\n",
        "\n",
        "        # Coordinates of the bounding box\n",
        "        x, y, w, h = box\n",
        "\n",
        "        # Draw the bounding box and label\n",
        "        color = (0, 255, 0)  # Green color for the bounding box\n",
        "        label = annotations_dict.get(class_id, 'Unknown')\n",
        "\n",
        "        # Draw the rectangle\n",
        "        cv2.rectangle(image, (x, y), (x + w, y + h), color, 2)\n",
        "        cv2.putText(image, f\"{label} {score:.2f}\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
        "\n",
        "    return image\n",
        "\n",
        "# Iterate over all frames in the folder\n",
        "frame_files = sorted(os.listdir(test_frames_path))\n",
        "for frame_file in frame_files:\n",
        "    frame_path = os.path.join(test_frames_path, frame_file)\n",
        "    image = preprocess_image(frame_path)\n",
        "\n",
        "    # Predict object detection on the frame\n",
        "    predictions = model.predict(image)\n",
        "\n",
        "\n",
        "    boxes = [(50, 50, 100, 100)]  # Example box\n",
        "    class_ids = [0]  # Example: 0 for phone, 1 for book\n",
        "    scores = [0.95]  # Confidence score\n",
        "\n",
        "    # Draw bounding boxes on the frame\n",
        "    image_with_boxes = draw_bounding_boxes(image[0], boxes, class_ids, scores)\n",
        "\n",
        "    # Save the result\n",
        "    output_image_path = os.path.join(output_path, f\"result_{frame_file}\")\n",
        "    cv2.imwrite(output_image_path, image_with_boxes)\n",
        "\n",
        "print(f\"Processed and saved frames with bounding boxes to {output_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZ3uXO_8FA8z",
        "outputId": "62025a65-f1f4-4c87-83be-c9fe45eb35f9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 552ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "Processed and saved frames with bounding boxes to /content/drive/MyDrive/computer vision/object_detection/test_output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image = cv2.imread('/content/drive/MyDrive/computer vision/object_detection/test_output/result_frame_0000.jpg')\n",
        "image"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 886
        },
        "id": "srk8aY6i2UVY",
        "outputId": "21ab101d-35ac-4a60-e098-686ddc14bf60"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0, 0, 0],\n",
              "        [0, 0, 0],\n",
              "        [0, 0, 0],\n",
              "        ...,\n",
              "        [0, 0, 0],\n",
              "        [0, 0, 0],\n",
              "        [0, 0, 0]],\n",
              "\n",
              "       [[0, 0, 0],\n",
              "        [0, 0, 0],\n",
              "        [0, 0, 0],\n",
              "        ...,\n",
              "        [0, 0, 0],\n",
              "        [0, 0, 0],\n",
              "        [0, 0, 0]],\n",
              "\n",
              "       [[0, 0, 0],\n",
              "        [0, 0, 0],\n",
              "        [0, 0, 0],\n",
              "        ...,\n",
              "        [0, 0, 0],\n",
              "        [0, 0, 0],\n",
              "        [0, 0, 0]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[0, 0, 0],\n",
              "        [0, 0, 0],\n",
              "        [0, 0, 0],\n",
              "        ...,\n",
              "        [0, 0, 0],\n",
              "        [0, 0, 0],\n",
              "        [0, 0, 0]],\n",
              "\n",
              "       [[0, 0, 0],\n",
              "        [0, 0, 0],\n",
              "        [0, 0, 0],\n",
              "        ...,\n",
              "        [0, 0, 0],\n",
              "        [0, 0, 0],\n",
              "        [0, 0, 0]],\n",
              "\n",
              "       [[0, 0, 0],\n",
              "        [0, 0, 0],\n",
              "        [0, 0, 0],\n",
              "        ...,\n",
              "        [0, 0, 0],\n",
              "        [0, 0, 0],\n",
              "        [0, 0, 0]]], dtype=uint8)"
            ],
            "text/html": [
              "<style>\n",
              "      .ndarray_repr .ndarray_raw_data {\n",
              "        display: none;\n",
              "      }\n",
              "      .ndarray_repr.show_array .ndarray_raw_data {\n",
              "        display: block;\n",
              "      }\n",
              "      .ndarray_repr.show_array .ndarray_image_preview {\n",
              "        display: none;\n",
              "      }\n",
              "      </style>\n",
              "      <div id=\"id-1c229852-f39b-42ba-9136-a1c32faa4ba1\" class=\"ndarray_repr\"><pre>ndarray (224, 224, 3) <button style=\"padding: 0 2px;\">show data</button></pre><img src=\"data:image/png;base64,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\" class=\"ndarray_image_preview\" /><pre class=\"ndarray_raw_data\">array([[[0, 0, 0],\n",
              "        [0, 0, 0],\n",
              "        [0, 0, 0],\n",
              "        ...,\n",
              "        [0, 0, 0],\n",
              "        [0, 0, 0],\n",
              "        [0, 0, 0]],\n",
              "\n",
              "       [[0, 0, 0],\n",
              "        [0, 0, 0],\n",
              "        [0, 0, 0],\n",
              "        ...,\n",
              "        [0, 0, 0],\n",
              "        [0, 0, 0],\n",
              "        [0, 0, 0]],\n",
              "\n",
              "       [[0, 0, 0],\n",
              "        [0, 0, 0],\n",
              "        [0, 0, 0],\n",
              "        ...,\n",
              "        [0, 0, 0],\n",
              "        [0, 0, 0],\n",
              "        [0, 0, 0]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[0, 0, 0],\n",
              "        [0, 0, 0],\n",
              "        [0, 0, 0],\n",
              "        ...,\n",
              "        [0, 0, 0],\n",
              "        [0, 0, 0],\n",
              "        [0, 0, 0]],\n",
              "\n",
              "       [[0, 0, 0],\n",
              "        [0, 0, 0],\n",
              "        [0, 0, 0],\n",
              "        ...,\n",
              "        [0, 0, 0],\n",
              "        [0, 0, 0],\n",
              "        [0, 0, 0]],\n",
              "\n",
              "       [[0, 0, 0],\n",
              "        [0, 0, 0],\n",
              "        [0, 0, 0],\n",
              "        ...,\n",
              "        [0, 0, 0],\n",
              "        [0, 0, 0],\n",
              "        [0, 0, 0]]], dtype=uint8)</pre></div><script>\n",
              "      (() => {\n",
              "      const titles = ['show data', 'hide data'];\n",
              "      let index = 0\n",
              "      document.querySelector('#id-1c229852-f39b-42ba-9136-a1c32faa4ba1 button').onclick = (e) => {\n",
              "        document.querySelector('#id-1c229852-f39b-42ba-9136-a1c32faa4ba1').classList.toggle('show_array');\n",
              "        index = (++index) % 2;\n",
              "        document.querySelector('#id-1c229852-f39b-42ba-9136-a1c32faa4ba1 button').textContent = titles[index];\n",
              "        e.preventDefault();\n",
              "        e.stopPropagation();\n",
              "      }\n",
              "      })();\n",
              "    </script>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    }
  ]
}